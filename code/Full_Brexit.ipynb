{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file, read in dataframe, correct columns, grouped by reduction, file write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['huge_query-20160713-edgesBrexit_10_16jun.csv', 'huge_query-20160713-edgesBrexit_24_30jun.csv', 'huge_query-20160713-edgesBrexit_27may_2jun.csv', 'huge_query-20160713-edgesBrexit_3_9jun.csv', 'huge_query-20160713-edgesBrexit_17_23jun.csv']\n",
      "['10_16jun', '24_30jun', '27may_2jun', '3_9jun', '17_23jun']\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "file_names = []\n",
    "for f in os.listdir('./Brexit_Data/'):\n",
    "    if f[-4:] == '.csv':\n",
    "        file_list.append(f)\n",
    "        file_names.append(re.findall(r'[0-9]+[a-z]*_[0-9].*.csv', f)[0][:-4])\n",
    "print(file_list)\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 10_16jun \n",
      "source_tweet_created_at,source,target,Red,Weight\n",
      "\n",
      "2016-06-10 05:01:01,daily_biz_news,business,Retweet,1\n",
      "\n",
      "2016-06-10 05:01:24,ahmdabdallah12,business,Reply,1\n",
      "\n",
      "Finished with 10_16jun\n",
      "Processing file 24_30jun \n",
      "source_tweet_created_at,source,target,Red,Weight\n",
      "\n",
      "2016-06-24 05:00:05,trendinaliaMX,trendinaliaMX,Reply,1\n",
      "\n",
      "2016-06-24 05:00:12,JoseGRojasZ,bolsamania,Retweet,1\n",
      "\n",
      "Finished with 24_30jun\n",
      "Processing file 27may_2jun \n",
      "source_tweet_created_at,source,target,Red,Weight\n",
      "\n",
      "2016-05-27 05:01:54,maca_13_9,BlueEyedSoulMan,Retweet,1\n",
      "\n",
      "2016-05-27 05:02:07,maca_13_9,yogs1961,Retweet,1\n",
      "\n",
      "Finished with 27may_2jun\n",
      "Processing file 3_9jun \n",
      "source_tweet_created_at,source,target,Red,Weight\n",
      "\n",
      "2016-06-03 05:01:13,bruce_bwkm,Stop_The_EU,Retweet,1\n",
      "\n",
      "2016-06-03 05:01:49,mistamark,Stop_The_EU,Retweet,1\n",
      "\n",
      "Finished with 3_9jun\n",
      "Processing file 17_23jun \n",
      "source_tweet_created_at,source,target,Red,Weight\n",
      "\n",
      "2016-06-17 05:02:05,shattered_c,bartle_booth,Retweet,1\n",
      "\n",
      "2016-06-17 05:02:52,REPORTER_47,DailyAgendaUK,Retweet,1\n",
      "\n",
      "Finished with 17_23jun\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(file_list)):\n",
    "    print('Processing file %s ' % file_names[i])\n",
    "    \n",
    "    # Read in the edges file\n",
    "    df_edges = pd.read_csv('./Brexit_Data/%s' % file_list[i])\n",
    "    \n",
    "    # Remove space from beginning of some of the columns for ease of use later\n",
    "    columns = df_edges.columns\n",
    "    renaming_dict = {}\n",
    "    for c in columns:\n",
    "        if c[0] == ' ':\n",
    "            renaming_dict[c] = c[1:]\n",
    "    df_edges = df_edges.rename(index=str, columns=renaming_dict)\n",
    "    \n",
    "    # Do grouped by first, save to file (reduces size) then read in new data frame\n",
    "    df_edges = df_edges[['source_tweet_created_at', 'source', 'target', 'Weight', 'Red']]\n",
    "    grouped = df_edges.groupby(['source_tweet_created_at','source', 'target', 'Red']).mean() # NOTE need to change \n",
    "                                                                            # here for different weightings\n",
    "\n",
    "    file_path = './edge_list_%s.csv' % file_names[i]\n",
    "    grouped.to_csv(file_path)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        k = 0\n",
    "        for line in f:\n",
    "            print(line)\n",
    "            k += 1\n",
    "            if k > 2:\n",
    "                break\n",
    "    \n",
    "    print('Finished with %s' % file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['27may_2jun', '24_30jun', '10_16jun', '3_9jun', '17_23jun']\n"
     ]
    }
   ],
   "source": [
    "# Put the may 27 - jun 2 file name first\n",
    "fn = file_names[2] \n",
    "file_names[2] = file_names[0]\n",
    "file_names[0] = fn\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_tweet_created_at          source           target  Weight      Red\n",
      "0     2016-05-27 05:01:54       maca_13_9  BlueEyedSoulMan       1  Retweet\n",
      "1     2016-05-27 05:02:07       maca_13_9         yogs1961       1  Retweet\n",
      "2     2016-05-27 05:03:47  TheJackieBrook         bbc5live       1  Retweet\n",
      "3     2016-05-27 05:03:49    BrentBicycle        TheFogeys       1  Retweet\n",
      "4     2016-05-27 05:05:42      JcPhilipot     RTenfrancais       1  Retweet\n",
      "finding first time:\n",
      "\tFirst time is 2016-05-27 05:01:54\n",
      "   source_tweet_created_at          source           target  Weight      Red\n",
      "0                        0       maca_13_9  BlueEyedSoulMan       1  Retweet\n",
      "1                       13       maca_13_9         yogs1961       1  Retweet\n",
      "2                      113  TheJackieBrook         bbc5live       1  Retweet\n",
      "3                      115    BrentBicycle        TheFogeys       1  Retweet\n",
      "4                      228      JcPhilipot     RTenfrancais       1  Retweet\n",
      "Overwriting ./edge_list_27may_2jun.csv\n",
      "  source_tweet_created_at          source           target  Weight      Red\n",
      "0     2016-06-24 05:00:05   trendinaliaMX    trendinaliaMX       1    Reply\n",
      "1     2016-06-24 05:00:12     JoseGRojasZ       bolsamania       1  Retweet\n",
      "2     2016-06-24 05:00:22        HCH_Hill     AidanKerrPol       1  Retweet\n",
      "3     2016-06-24 05:00:22  SuperSheffield    SheffieldStar       1  Retweet\n",
      "4     2016-06-24 05:00:27     MrsMooville  michaeltinmouth       1  Retweet\n",
      "   source_tweet_created_at          source           target  Weight      Red\n",
      "0                  2419091   trendinaliaMX    trendinaliaMX       1    Reply\n",
      "1                  2419098     JoseGRojasZ       bolsamania       1  Retweet\n",
      "2                  2419108        HCH_Hill     AidanKerrPol       1  Retweet\n",
      "3                  2419108  SuperSheffield    SheffieldStar       1  Retweet\n",
      "4                  2419113     MrsMooville  michaeltinmouth       1  Retweet\n",
      "Overwriting ./edge_list_24_30jun.csv\n",
      "  source_tweet_created_at          source    target  Weight      Red\n",
      "0     2016-06-10 05:01:01  daily_biz_news  business       1  Retweet\n",
      "1     2016-06-10 05:01:24  ahmdabdallah12  business       1    Reply\n",
      "2     2016-06-10 05:01:26       awilkinso  business       1  Retweet\n",
      "3     2016-06-10 05:01:47     wmiddelkoop  business       1  Retweet\n",
      "4     2016-06-10 05:01:54          MagsRG   itvnews       1  Retweet\n",
      "   source_tweet_created_at          source    target  Weight      Red\n",
      "0                  1209547  daily_biz_news  business       1  Retweet\n",
      "1                  1209570  ahmdabdallah12  business       1    Reply\n",
      "2                  1209572       awilkinso  business       1  Retweet\n",
      "3                  1209593     wmiddelkoop  business       1  Retweet\n",
      "4                  1209600          MagsRG   itvnews       1  Retweet\n",
      "Overwriting ./edge_list_10_16jun.csv\n",
      "  source_tweet_created_at         source       target  Weight      Red\n",
      "0     2016-06-03 05:01:13     bruce_bwkm  Stop_The_EU       1  Retweet\n",
      "1     2016-06-03 05:01:49      mistamark  Stop_The_EU       1  Retweet\n",
      "2     2016-06-03 05:02:17    chiefstoker  Stop_The_EU       1  Retweet\n",
      "3     2016-06-03 05:02:57     AndiNeglia  Stop_The_EU       1  Retweet\n",
      "4     2016-06-03 05:03:17  VentYerSpleen  Stop_The_EU       1  Retweet\n",
      "   source_tweet_created_at         source       target  Weight      Red\n",
      "0                   604759     bruce_bwkm  Stop_The_EU       1  Retweet\n",
      "1                   604795      mistamark  Stop_The_EU       1  Retweet\n",
      "2                   604823    chiefstoker  Stop_The_EU       1  Retweet\n",
      "3                   604863     AndiNeglia  Stop_The_EU       1  Retweet\n",
      "4                   604883  VentYerSpleen  Stop_The_EU       1  Retweet\n",
      "Overwriting ./edge_list_3_9jun.csv\n",
      "  source_tweet_created_at           source         target  Weight      Red\n",
      "0     2016-06-17 05:02:05      shattered_c   bartle_booth       1  Retweet\n",
      "1     2016-06-17 05:02:52      REPORTER_47  DailyAgendaUK       1  Retweet\n",
      "2     2016-06-17 05:03:18   jageradeheraus           wiwo       1    Reply\n",
      "3     2016-06-17 05:03:27  shailendra_nair       htTweets       1  Retweet\n",
      "4     2016-06-17 05:03:50      mixalis_rak        prezatv       1  Retweet\n",
      "   source_tweet_created_at           source         target  Weight      Red\n",
      "0                  1814411      shattered_c   bartle_booth       1  Retweet\n",
      "1                  1814458      REPORTER_47  DailyAgendaUK       1  Retweet\n",
      "2                  1814484   jageradeheraus           wiwo       1    Reply\n",
      "3                  1814493  shailendra_nair       htTweets       1  Retweet\n",
      "4                  1814516      mixalis_rak        prezatv       1  Retweet\n",
      "Overwriting ./edge_list_17_23jun.csv\n"
     ]
    }
   ],
   "source": [
    "# Get names and node id transformation, along with transformation of date_time to int time \n",
    "names = []\n",
    "_format= \"%Y-%m-%d %H:%M:%S\"  # format for transforming date strings to integer times\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    file_path = './edge_list_%s.csv' % file_names[i]\n",
    "    df_edges = pd.read_csv(file_path)\n",
    "    \n",
    "    # add the unique names from this data frame\n",
    "    names_i = list(np.union1d(df_edges.target.unique(), df_edges.source.unique()))\n",
    "    names = list(np.union1d(names, names_i))\n",
    "    \n",
    "    # sort by times\n",
    "    edge_list = df_edges[['source_tweet_created_at', 'source', 'target', 'Weight', 'Red']]\n",
    "    edge_list_sort = edge_list.sort_values(by=['source_tweet_created_at'])\n",
    "    print(edge_list_sort.head())\n",
    "    \n",
    "    # get the times column for transforming\n",
    "    num_times = edge_list_sort['source_tweet_created_at']\n",
    "    \n",
    "    # find min time (in 27 may - 2 jun)\n",
    "    if file_names[i] == '27may_2jun':\n",
    "        print('finding first time:')\n",
    "        ft = num_times[0]\n",
    "        print('\\tFirst time is %s' % ft)\n",
    "        first_time = int(time.mktime(datetime.datetime.strptime(ft, _format).timetuple()))\n",
    "        \n",
    "        # define function to transform all other date strings to ordered times\n",
    "        minutes = lambda s : int(time.mktime(datetime.datetime.strptime(s, _format).timetuple())) - first_time\n",
    "    \n",
    "    new_num_times = num_times.apply(minutes)\n",
    "    edge_list_sort['source_tweet_created_at'] = new_num_times # replace the source_tweet_created_at with int times calculated\n",
    "    \n",
    "    print(edge_list_sort.head())\n",
    "    \n",
    "    # write new files with new ordering, OVER THE OLD FILES. (for saving space, otherwise would've done in dictionaries)\n",
    "    print('Overwriting %s' % file_path)\n",
    "    edge_list_sort.to_csv(file_path, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing file nodes.txt\n"
     ]
    }
   ],
   "source": [
    "# Create the transformation from node_id to Name and vv\n",
    "n_nodes = len(names)\n",
    "node_id2Name = dict(zip(range(n_nodes), names))\n",
    "name2Node_id = dict(zip(names, range(n_nodes)))  # for use in this script since will use later on\n",
    "\n",
    "with open('nodes.txt', 'w') as f:\n",
    "    f.write('node_id,name\\n')\n",
    "    for i in range(n_nodes):\n",
    "        line = ','.join([str(i), str(names[i])])\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print('Done writing file nodes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source_tweet_created_at  Weight      Red  source_id  target_id\n",
      "0                        0       1  Retweet    1045153      85824\n",
      "1                       13       1  Retweet    1045153    1314918\n",
      "2                      113       1  Retweet     600269     735667\n",
      "3                      115       1  Retweet      91437     599474\n",
      "4                      228       1  Retweet     296748     500483\n",
      "   source_tweet_created_at  source_id  target_id  Weight      Red\n",
      "0                        0    1045153      85824       1  Retweet\n",
      "1                       13    1045153    1314918       1  Retweet\n",
      "2                      113     600269     735667       1  Retweet\n",
      "3                      115      91437     599474       1  Retweet\n",
      "4                      228     296748     500483       1  Retweet\n",
      "Overwriting ./edge_list_27may_2jun.csv\n",
      "   source_tweet_created_at  Weight      Red  source_id  target_id\n",
      "0                  2419091       1    Reply    1277170    1277170\n",
      "1                  2419098       1  Retweet     314598     752544\n",
      "2                  2419108       1  Retweet     242281      28099\n",
      "3                  2419108       1  Retweet     582763     555684\n",
      "4                  2419113       1  Retweet     433247    1076437\n",
      "   source_tweet_created_at  source_id  target_id  Weight      Red\n",
      "0                  2419091    1277170    1277170       1    Reply\n",
      "1                  2419098     314598     752544       1  Retweet\n",
      "2                  2419108     242281      28099       1  Retweet\n",
      "3                  2419108     582763     555684       1  Retweet\n",
      "4                  2419113     433247    1076437       1  Retweet\n",
      "Overwriting ./edge_list_24_30jun.csv\n",
      "   source_tweet_created_at  Weight      Red  source_id  target_id\n",
      "0                  1209547       1  Retweet     805893     761043\n",
      "1                  1209570       1    Reply     688888     761043\n",
      "2                  1209572       1  Retweet     728896     761043\n",
      "3                  1209593       1  Retweet    1305127     761043\n",
      "4                  1209600       1  Retweet     388917     942100\n",
      "   source_tweet_created_at  source_id  target_id  Weight      Red\n",
      "0                  1209547     805893     761043       1  Retweet\n",
      "1                  1209570     688888     761043       1    Reply\n",
      "2                  1209572     728896     761043       1  Retweet\n",
      "3                  1209593    1305127     761043       1  Retweet\n",
      "4                  1209600     388917     942100       1  Retweet\n",
      "Overwriting ./edge_list_10_16jun.csv\n",
      "   source_tweet_created_at  Weight      Red  source_id  target_id\n",
      "0                   604759       1  Retweet     758563     578931\n",
      "1                   604795       1  Retweet    1084026     578931\n",
      "2                   604823       1  Retweet     781241     578931\n",
      "3                   604863       1  Retweet      44573     578931\n",
      "4                   604883       1  Retweet     630893     578931\n",
      "   source_tweet_created_at  source_id  target_id  Weight      Red\n",
      "0                   604759     758563     578931       1  Retweet\n",
      "1                   604795    1084026     578931       1  Retweet\n",
      "2                   604823     781241     578931       1  Retweet\n",
      "3                   604863      44573     578931       1  Retweet\n",
      "4                   604883     630893     578931       1  Retweet\n",
      "Overwriting ./edge_list_3_9jun.csv\n",
      "   source_tweet_created_at  Weight      Red  source_id  target_id\n",
      "0                  1814411       1  Retweet    1213364     734454\n",
      "1                  1814458       1  Retweet     498446     146299\n",
      "2                  1814484       1    Reply     947348    1304781\n",
      "3                  1814493       1  Retweet    1211638     922987\n",
      "4                  1814516       1  Retweet    1084632    1155901\n",
      "   source_tweet_created_at  source_id  target_id  Weight      Red\n",
      "0                  1814411    1213364     734454       1  Retweet\n",
      "1                  1814458     498446     146299       1  Retweet\n",
      "2                  1814484     947348    1304781       1    Reply\n",
      "3                  1814493    1211638     922987       1  Retweet\n",
      "4                  1814516    1084632    1155901       1  Retweet\n",
      "Overwriting ./edge_list_17_23jun.csv\n"
     ]
    }
   ],
   "source": [
    "# Transform Twitter names to node id's as determined by above \n",
    "for i in range(len(file_names)):\n",
    "    # read in data frame\n",
    "    file_path = './edge_list_%s.csv' % file_names[i]\n",
    "    df_edges = pd.read_csv(file_path)\n",
    "    \n",
    "    # source id's\n",
    "    source_col_ids = df_edges.source.apply(lambda x: name2Node_id[x])\n",
    "    df_edges['source_id'] = source_col_ids\n",
    "    \n",
    "    # target id's \n",
    "    target_col_ids = df_edges.target.apply(lambda x: name2Node_id[x])\n",
    "    df_edges['target_id'] = target_col_ids\n",
    "    \n",
    "    # drop old columns, \n",
    "    df_edges = df_edges.drop(columns = ['source', 'target'], axis=1)\n",
    "    df_edges = df_edges.reindex(columns = ['source_tweet_created_at', 'source_id', 'target_id', 'Weight', 'Red'])\n",
    "    \n",
    "    # write new files with new ordering, OVER THE OLD FILES. (for saving space, otherwise would've done in dictionaries)\n",
    "    print('Overwriting %s' % file_path)\n",
    "    df_edges.to_csv(file_path, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_tweet_created_at</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1209547</td>\n",
       "      <td>805893</td>\n",
       "      <td>761043</td>\n",
       "      <td>1</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1209570</td>\n",
       "      <td>688888</td>\n",
       "      <td>761043</td>\n",
       "      <td>1</td>\n",
       "      <td>Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1209572</td>\n",
       "      <td>728896</td>\n",
       "      <td>761043</td>\n",
       "      <td>1</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1209593</td>\n",
       "      <td>1305127</td>\n",
       "      <td>761043</td>\n",
       "      <td>1</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1209600</td>\n",
       "      <td>388917</td>\n",
       "      <td>942100</td>\n",
       "      <td>1</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_tweet_created_at  source_id  target_id  Weight      Red\n",
       "0                  1209547     805893     761043       1  Retweet\n",
       "1                  1209570     688888     761043       1    Reply\n",
       "2                  1209572     728896     761043       1  Retweet\n",
       "3                  1209593    1305127     761043       1  Retweet\n",
       "4                  1209600     388917     942100       1  Retweet"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = './edge_list_%s.csv' % file_names[2]\n",
    "df = pd.read_csv(fp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Edge Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./full_data/edge_list_27may_2jun.csv \n",
      "Data frame 27may_2jun took 64.073657 seconds\n",
      "Working on ./full_data/edge_list_24_30jun.csv \n",
      "Data frame 24_30jun took 74.299402 seconds\n",
      "Working on ./full_data/edge_list_10_16jun.csv \n",
      "Data frame 10_16jun took 129.985792 seconds\n",
      "Working on ./full_data/edge_list_3_9jun.csv \n",
      "Data frame 3_9jun took 99.577079 seconds\n",
      "Working on ./full_data/edge_list_17_23jun.csv \n",
      "Data frame 17_23jun took 329.422978 seconds\n"
     ]
    }
   ],
   "source": [
    "# instantiate the dictionary that will hold all of the edge lists\n",
    "full_edge_dict = {}\n",
    "\n",
    "for k in range(len(file_names)):\n",
    "    # read in data frame\n",
    "    #file_path = './full_data/edge_list_%s.csv' % file_names[k]\n",
    "    file_path = './full_data/edge_list_%s.csv' % file_names[k]\n",
    "    print('Working on %s ' % file_path)\n",
    "    df_edges = pd.read_csv(file_path)\n",
    "    \n",
    "    #count = 0\n",
    "    tic =time.clock()\n",
    "    for row in df_edges.iterrows():\n",
    "        r = row[1]\n",
    "        t, i, j, w = r.source_tweet_created_at, int(r.source_id), int(r.target_id), float(r.Weight)\n",
    "        if (i,j) not in full_edge_dict.keys():\n",
    "            full_edge_dict[(i,j)] = [(t,w)]\n",
    "        else:\n",
    "            full_edge_dict[(i,j)].append((t,w))\n",
    "        #count += 1\n",
    "        #if count > 5:\n",
    "        #    break\n",
    "    toc = time.clock()\n",
    "    print('Data frame %s took %f seconds' % (file_names[k],(toc - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to pickle file\n",
    "fpath = './full_data/full_edge_dict.pkl'\n",
    "#fpath = './full_edge_dict.pkl'\n",
    "f = open(fpath,\"wb\")\n",
    "pickle.dump(full_edge_dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
